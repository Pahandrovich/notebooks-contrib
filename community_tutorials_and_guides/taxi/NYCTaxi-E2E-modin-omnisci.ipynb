{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Predicting NYC Taxi Fares with Modin on Omnisci"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is a notebook originally written for Rapids but converted to use Modin on Omnisci."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%matplotlib inline\r\n",
    "import glob\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import socket, time\r\n",
    "import modin.pandas as modin_omni_pd\r\n",
    "import xgboost as xgb\r\n",
    "\r\n",
    "#To install Holoviews and hvplot\r\n",
    "#conda install -c conda-forge holoviews\r\n",
    "#conda install -c pyviz hvplot\r\n",
    "import holoviews as hv\r\n",
    "from holoviews import opts\r\n",
    "import numpy as np\r\n",
    "import hvplot.pandas\r\n",
    "import hvplot.dask\r\n",
    "hv.extension('bokeh')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Inspecting the Data\n",
    "\n",
    "We'll use Modin on Omnisci to load and parse all CSV files into a DataFrame. It makes it 30 files overall."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Cleanup\n",
    "\n",
    "As usual, the data needs to be massaged a bit before we can start adding features that are useful to an ML model.\n",
    "\n",
    "For example, in the 2014 taxi CSV files, there are `pickup_datetime` and `dropoff_datetime` columns. The 2015 CSVs have `tpep_pickup_datetime` and `tpep_dropoff_datetime`, which are the same columns. One year has `rate_code`, and another `RateCodeID`.\n",
    "\n",
    "Also, some CSV files have column names with extraneous spaces in them.\n",
    "\n",
    "Worst of all, starting in the July 2016 CSVs, pickup & dropoff latitude and longitude data were replaced by location IDs, making the second half of the year useless to us.\n",
    "\n",
    "We'll do a little string manipulation, column renaming, and concatenating of DataFrames to sidestep the problems."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Dictionary of required columns and their datatypes\r\n",
    "must_haves = {\r\n",
    "     'pickup_datetime': 'datetime64[s]',\r\n",
    "     'dropoff_datetime': 'datetime64[s]',\r\n",
    "     'passenger_count': 'int32',\r\n",
    "     'trip_distance': 'float32',\r\n",
    "     'pickup_longitude': 'float32',\r\n",
    "     'pickup_latitude': 'float32',\r\n",
    "     'rate_code': 'int32',\r\n",
    "     'dropoff_longitude': 'float32',\r\n",
    "     'dropoff_latitude': 'float32',\r\n",
    "     'fare_amount': 'float32'\r\n",
    "    }"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def clean(ddf, must_haves):\r\n",
    "    # replace the extraneous spaces in column names and lower the font type\r\n",
    "    tmp = {col:col.strip().lower() for col in list(ddf.columns)}\r\n",
    "    ddf = ddf.rename(columns=tmp)\r\n",
    "\r\n",
    "    ddf = ddf.rename(columns={\r\n",
    "        'tpep_pickup_datetime': 'pickup_datetime',\r\n",
    "        'tpep_dropoff_datetime': 'dropoff_datetime',\r\n",
    "        'ratecodeid': 'rate_code'\r\n",
    "    })\r\n",
    "    \r\n",
    "    for col in ddf.columns:\r\n",
    "        if col not in must_haves:\r\n",
    "            ddf = ddf.drop(columns=col)\r\n",
    "            continue\r\n",
    "        if ddf[col].dtype == 'object':\r\n",
    "            ddf[col] = ddf[col].fillna('-1')\r\n",
    "    \r\n",
    "    return ddf"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "base_path = '/localdisk/benchmark_datasets/yellow-taxi-dataset/'\r\n",
    "\r\n",
    "df_2014 = modin_omni_pd.concat([\r\n",
    "    clean(modin_omni_pd.read_csv(x, parse_dates=[' pickup_datetime', ' dropoff_datetime']), must_haves)\r\n",
    "    for x in glob.glob(base_path+'2014/yellow_*.csv')], ignore_index=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_2014.dtypes"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<b> NOTE: </b>We will realize that some of 2015 data has column name as `RateCodeID` and others have `RatecodeID`. When we rename the columns in the clean function, it internally doesn't pass meta while calling map_partitions(). This leads to the error of column name mismatch in the returned data. For this reason, we will call the clean function with map_partition and pass the meta to it. Here is the link to the bug created for that: https://github.com/rapidsai/cudf/issues/5413 "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_2014.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We still have 2015 and the first half of 2016's data to read and clean. Let's increase our dataset."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_2015 = modin_omni_pd.concat([\r\n",
    "    clean(modin_omni_pd.read_csv(x, parse_dates=['tpep_pickup_datetime', 'tpep_dropoff_datetime']), must_haves)\r\n",
    "    for x in glob.glob(base_path + '2015/yellow_*.csv')], ignore_index=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_2015.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Handling 2016's Mid-Year Schema Change"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In 2016, only January - June CSVs have the columns we need. If we try to read base_path+2016/yellow_*.csv, Dask will not appreciate having differing schemas in the same DataFrame.\n",
    "\n",
    "Instead, we'll need to create a list of the valid months and read them independently."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "months = [str(x).rjust(2, '0') for x in range(1, 7)]\r\n",
    "valid_files = [base_path+'2016/yellow_tripdata_2016-'+month+'.csv' for month in months]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#read & clean 2016 data and concat all DFs\r\n",
    "df_2016 = modin_omni_pd.concat([\r\n",
    "    clean(modin_omni_pd.read_csv(x, parse_dates=['tpep_pickup_datetime', 'tpep_dropoff_datetime']), must_haves)\r\n",
    "    for x in valid_files], ignore_index=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#concatenate multiple DataFrames into one bigger one\r\n",
    "taxi_df = modin_omni_pd.concat([df_2014, df_2015, df_2016], ignore_index=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# taxi_df = taxi_df.persist()\r\n",
    "taxi_df.dtypes"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "taxi_df.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here, we are checking out if there are any non-sensical records and outliers, and in such case, we need to remove them from the dataset."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# check out if there is any negative total trip time\r\n",
    "taxi_df[taxi_df.dropoff_datetime <= taxi_df.pickup_datetime].head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# check out if there is any abnormal data where trip distance is short, but the fare is very high.\r\n",
    "taxi_df[(taxi_df.trip_distance < 10) & (taxi_df.fare_amount > 300)].head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# check out if there is any abnormal data where trip distance is long, but the fare is very low.\r\n",
    "taxi_df[(taxi_df.trip_distance > 50) & (taxi_df.fare_amount < 50)].head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Using only 2016-01 data for visuals.\r\n",
    "#taxi_df_cdf = clean(cudf.read_csv(valid_files[0]),must_haves)\r\n",
    "\r\n",
    "#Using entire 2016 data for visualization\r\n",
    "#taxi_df_cdf = taxi_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The plot below visualizes the histogram of trip_distance and we can see some abnormal trip_distance values for some records. Taking this and also the NYC map coordinates into consideration, we will only select records where tripdistance < 500 miles."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\r\n",
    "#Histogram using cupy and Holoviews\r\n",
    "# frequencies, edges = cupy.histogram(x=cupy.array(taxi_df_cdf[\"trip_distance\"]) , bins=20)\r\n",
    "# hist = hv.Histogram((np.array(edges.tolist()), np.array(frequencies.tolist())))\r\n",
    "\r\n",
    "#Histogram using hvplot\r\n",
    "hist = taxi_df_cdf._to_pandas().hvplot.hist(\"trip_distance\", bins=20, bin_range=(0, 10))\r\n",
    "\r\n",
    "#Customizing the plot\r\n",
    "hist.opts(xlabel=\"trip distance (miles)\",ylabel=\"count\",color=\"green\",width=900, height=400)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Similarly, the plot below visualizes the histogram of fare_amount and we can see some abnormal fare_amount values for some records. Taking this and also the NYC map coordinates into consideration, we will only select records where fare_amount < 500$."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\r\n",
    "#Histogram using cupy and Holoviews\r\n",
    "# frequencies, edges = cupy.histogram(x=cupy.array(taxi_df_cdf[\"fare_amount\"]) , bins=20)\r\n",
    "# hist = hv.Histogram((np.array(edges.tolist()), np.array(frequencies.tolist())))\r\n",
    "\r\n",
    "#Histogram using hvplot\r\n",
    "hist = taxi_df_cdf._to_pandas().hvplot.hist(\"fare_amount\", bins=20, bin_range=(0, 50))\r\n",
    "\r\n",
    "#Customizing the plot\r\n",
    "hist.opts(xlabel=\"fare amount ($)\",ylabel=\"count\",color=\"green\",width=900, height=400)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\r\n",
    "# Plot the number of passengers per trip. We'll remove the records where passenger_count > 5.\r\n",
    "# Plotting using Holoviews\r\n",
    "#bar = hv.Bars(taxi_df_cdf.groupby(\"passenger_count\").size().to_frame().rename(columns={0:\"count\"}))\r\n",
    "\r\n",
    "# Plotting using hvplot\r\n",
    "df_bar = taxi_df_cdf.groupby(\"passenger_count\").size().to_frame().rename(columns={0:\"count\"}).reset_index()\r\n",
    "bar = df_bar._to_pandas().hvplot.bar(x=\"passenger_count\",y=\"count\")\r\n",
    "\r\n",
    "#Customizing the plot\r\n",
    "bar.opts(color=\"green\",width=900, height=400)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "EDA visuals and additional analysis yield the filter logic below."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "taxi_df.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# apply a list of filter conditions to throw out records with missing or outlier values\r\n",
    "taxi_df = taxi_df[\r\n",
    "    (taxi_df.fare_amount > 1) &\r\n",
    "    (taxi_df.fare_amount < 500) &\r\n",
    "    (taxi_df.passenger_count > 0) &\r\n",
    "    (taxi_df.passenger_count < 6) &\r\n",
    "    (taxi_df.pickup_longitude > -75) &\r\n",
    "    (taxi_df.pickup_longitude < -73) &\r\n",
    "    (taxi_df.dropoff_longitude > -75) &\r\n",
    "    (taxi_df.dropoff_longitude < -73) &\r\n",
    "    (taxi_df.pickup_latitude > 40) &\r\n",
    "    (taxi_df.pickup_latitude < 42) &\r\n",
    "    (taxi_df.dropoff_latitude > 40) &\r\n",
    "    (taxi_df.dropoff_latitude < 42) &\r\n",
    "    (taxi_df.trip_distance > 0) &\r\n",
    "    (taxi_df.trip_distance < 500) &\r\n",
    "    ((taxi_df.trip_distance <= 50) | (taxi_df.fare_amount >= 50)) &\r\n",
    "    ((taxi_df.trip_distance >= 10) | (taxi_df.fare_amount <= 300)) &\r\n",
    "    (taxi_df.dropoff_datetime > taxi_df.pickup_datetime)]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# reset_index and drop index column\r\n",
    "taxi_df = taxi_df.reset_index(drop=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Adding Interesting Features\n",
    "\n",
    "We'll use a Euclidean Distance calculation to find total trip distance, and extract additional useful variables from the datetime fields."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## add features\r\n",
    "taxi_df['day'] = taxi_df['pickup_datetime'].dt.day\r\n",
    "\r\n",
    "#calculate the time difference between dropoff and pickup.\r\n",
    "taxi_df['diff'] = taxi_df['dropoff_datetime'].astype('int64') - taxi_df['pickup_datetime'].astype('int64')\r\n",
    "\r\n",
    "taxi_df['pickup_latitude_r'] = taxi_df['pickup_latitude']//.01*.01\r\n",
    "taxi_df['pickup_longitude_r'] = taxi_df['pickup_longitude']//.01*.01\r\n",
    "taxi_df['dropoff_latitude_r'] = taxi_df['dropoff_latitude']//.01*.01\r\n",
    "taxi_df['dropoff_longitude_r'] = taxi_df['dropoff_longitude']//.01*.01\r\n",
    "\r\n",
    "taxi_df = taxi_df.drop('pickup_datetime', axis=1)\r\n",
    "taxi_df = taxi_df.drop('dropoff_datetime', axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "taxi_df.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dlon = taxi_df['dropoff_longitude'] - taxi_df['pickup_longitude']\r\n",
    "dlat = taxi_df['dropoff_latitude'] - taxi_df['pickup_latitude']\r\n",
    "taxi_df['e_distance'] = dlon * dlon + dlat * dlat"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "taxi_df.dtypes"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "taxi_df.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pick a Training Set\n",
    "\n",
    "Let's imagine you're making a trip to New York on the 25th and want to build a model to predict what fare prices will be like the last few days of the month based on the first part of the month. We'll use a query expression to identify the `day` of the month to use to divide the data into train and test sets."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#since we calculated the h_distance let's drop the trip_distance column, and then do model training with XGB.\r\n",
    "taxi_df = taxi_df.drop('trip_distance', axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# this is the original data partition for train and test sets.\r\n",
    "X_train = taxi_df[taxi_df.day < 25]\r\n",
    "\r\n",
    "# create a Y_train ddf with just the target variable\r\n",
    "Y_train = X_train[['fare_amount']]\r\n",
    "# drop the target variable from the training ddf\r\n",
    "X_train = X_train[X_train.columns.difference(['fare_amount'])]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train the XGBoost Regression Model\n",
    "\n",
    "The wall time output below indicates how long it took to train an XGBoost model over the training set."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_train.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_train.dtypes"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "Y_train.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dtrain = xgb.DMatrix(X_train, Y_train)"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\r\n",
    "\r\n",
    "trained_model = xgb.train({\r\n",
    "    'learning_rate': 0.3,\r\n",
    "    'max_depth': 8,\r\n",
    "    'objective': 'reg:squarederror',\r\n",
    "    'subsample': 0.6,\r\n",
    "    'gamma': 1,\r\n",
    "    'silent': True,\r\n",
    "    'verbose_eval': True,\r\n",
    "    'tree_method':'hist'\r\n",
    "    },\r\n",
    "    dtrain,\r\n",
    "    num_boost_round=100, evals=[(dtrain, 'train')])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ax = xgb.plot_importance(trained_model, height=0.8, max_num_features=10, importance_type=\"gain\")\r\n",
    "ax.grid(False, axis=\"y\")\r\n",
    "ax.set_title('Estimated feature importance')\r\n",
    "ax.set_xlabel('importance')\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# How Good is Our Model?\n",
    "\n",
    "Now that we have a trained model, we need to test it with the 25% of records we held out."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_test = taxi_df[taxi_df.day >= 25]\r\n",
    "\r\n",
    "# Create Y_test with just the fare amount\r\n",
    "Y_test = X_test[['fare_amount']]\r\n",
    "\r\n",
    "# Drop the fare amount from X_test\r\n",
    "X_test = X_test[X_test.columns.difference(['fare_amount'])]\r\n",
    "\r\n",
    "# display test set size\r\n",
    "X_test.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Calculate Prediction"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# generate predictions on the test set\r\n",
    "booster = trained_model\r\n",
    "prediction = modin_omni_pd.Series(booster.predict(xgb.DMatrix(X_test)))\r\n",
    "prediction.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# prediction = prediction.map_partitions(lambda part: cudf.Series(part)).reset_index(drop=True)\r\n",
    "actual = Y_test['fare_amount'].reset_index(drop=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "prediction.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "actual.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Calculate RMSE\r\n",
    "squared_error = ((prediction-actual)**2)\r\n",
    "\r\n",
    "# compute the actual RMSE over the full test set\r\n",
    "np.sqrt(squared_error.mean())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sklearn part"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Normalize data\r\n",
    "To get a better ridge regression model, we need to normalize the data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\r\n",
    "scaler_x = MinMaxScaler()\r\n",
    "scaler_y = StandardScaler()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "scaler_x.fit(X_train)\r\n",
    "X_train = scaler_x.transform(X_train)\r\n",
    "X_test = scaler_x.transform(X_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "scaler_y.fit(Y_train.to_numpy().reshape(-1, 1))\r\n",
    "Y_train = scaler_y.transform(Y_train.to_numpy().reshape(-1, 1)).ravel()\r\n",
    "Y_test = scaler_y.transform(Y_test.to_numpy().reshape(-1, 1)).ravel()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train Ridge regression model with  Intel® Extension for Scikit-learn*\r\n",
    "Patching is enabled by adding two lines of code:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearnex import patch_sklearn\r\n",
    "patch_sklearn()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.linear_model import Ridge\r\n",
    "from sklearn.metrics import mean_squared_error"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\r\n",
    "model_r_p = Ridge(random_state=123)\r\n",
    "model_r_p.fit(X_train, Y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train Ridge regression model without  Intel® Extension for Scikit-learn*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearnex import unpatch_sklearn\r\n",
    "unpatch_sklearn()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.linear_model import Ridge"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\r\n",
    "model_r_s = Ridge(random_state=123)\r\n",
    "model_r_s.fit(X_train, Y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Let's look MSE metric of Our Models"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y_pred_p = model_r_p.predict(X_test)\r\n",
    "y_pred_s = model_r_s.predict(X_test)\r\n",
    "\r\n",
    "mse_r_p = mean_squared_error(Y_test, y_pred_p)\r\n",
    "mse_r_s = mean_squared_error(Y_test, y_pred_s)\r\n",
    "\r\n",
    "print(f'MSE of pached Ridge: {mse_r_p}')\r\n",
    "print(f'MSE of unpached Ridge: {mse_r_s}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**As we can see the model with Intel® Extension for Scikit-learn learns much faster and has the same mse. In less time we get a model with the same quality**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Xgboost version comparison\r\n",
    "xgboost 1.4.2  \r\n",
    "Let's to campare different xgboost versions without and with Intel® optimizations"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import train_test_split\r\n",
    "x_train, x_val, y_train, y_val = train_test_split(X_train, Y_train, test_size=0.1, random_state=123)\r\n",
    "print(f\"x_train.shape = {x_train.shape}\")\r\n",
    "print(f\"y_train.shape = {y_train.shape}\")\r\n",
    "print(f\"x_val.shape = {x_val.shape}\")\r\n",
    "print(f\"y_val.shape = {y_val.shape}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We use the xgboost Regression model with default parameters and hist method"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model_xgb = xgb.XGBRegressor(tree_method='hist', n_jobs=-1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\r\n",
    "model_xgb = model_xgb.fit(x_train, y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\r\n",
    "xgb_predictions = model_xgb.predict(x_val)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mse = mean_squared_error(y_val, xgb_predictions)\r\n",
    "mse"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Xgboost 0.81 without Intel® optimizations provide:\r\n",
    "- Training time **2.84 times** slower than **xgboost 1.4.2**\r\n",
    "- Prediction time **3.07 times** slower than **xgboost 1.4.2**\r\n",
    "- MSE metric worse than xgboost 1.4.2: **4.293**\r\n",
    "  \r\n",
    "Work on accelerating training and predicting xgboost continues."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Xgboost with large dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Having big data, we cannot put it in the memory of the video card. For example, 6 months of the nyctaxi dataset no longer fit into the GPU.  \r\n",
    "We can put a lot more data on the CPU, for example, let's take 6 months of each year."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "months = [str(x).rjust(2, '0') for x in range(2, 7)]\r\n",
    "valid_files = [base_path+'2014/yellow_tripdata_2014-'+month+'.csv' for month in months]\r\n",
    "\r\n",
    "df_2014 = clean(df_2014, must_haves)\r\n",
    "df_2014_halfyear = [df_2014]\r\n",
    "for path in valid_files:\r\n",
    "    mounth_data = modin_omni_pd.read_csv(path, parse_dates=[' pickup_datetime', ' dropoff_datetime'])\r\n",
    "    mounth_data_cleaned = clean(mounth_data, must_haves)\r\n",
    "    df_2014_halfyear.append(mounth_data_cleaned)\r\n",
    "\r\n",
    "taxi_df_2014 = modin_omni_pd.concat(df_2014_halfyear)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "months = [str(x).rjust(2, '0') for x in range(2, 7)]\r\n",
    "valid_files = [base_path+'2015/yellow_tripdata_2015-'+month+'.csv' for month in months]\r\n",
    "\r\n",
    "df_2015 = clean(df_2015, must_haves)\r\n",
    "df_2015_halfyear = [df_2015]\r\n",
    "for path in valid_files:\r\n",
    "    mounth_data = modin_omni_pd.read_csv(path, parse_dates=['tpep_pickup_datetime', 'tpep_dropoff_datetime'])\r\n",
    "    mounth_data_cleaned = clean(mounth_data, must_haves)\r\n",
    "    df_2015_halfyear.append(mounth_data_cleaned)\r\n",
    "\r\n",
    "taxi_df_2015 = modin_omni_pd.concat(df_2015_halfyear)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "months = [str(x).rjust(2, '0') for x in range(2, 7)]\r\n",
    "valid_files = [base_path+'2016/yellow_tripdata_2016-'+month+'.csv' for month in months]\r\n",
    "\r\n",
    "df_2016 = clean(df_2016, must_haves)\r\n",
    "df_2016_halfyear = [df_2016]\r\n",
    "for path in valid_files:\r\n",
    "    mounth_data = modin_omni_pd.read_csv(path, parse_dates=['tpep_pickup_datetime', 'tpep_dropoff_datetime'])\r\n",
    "    mounth_data_cleaned = clean(mounth_data, must_haves)\r\n",
    "    df_2016_halfyear.append(mounth_data_cleaned)\r\n",
    "\r\n",
    "taxi_df_2016 = modin_omni_pd.concat(df_2016_halfyear)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "taxi_df = modin_omni_pd.concat([taxi_df_2014, taxi_df_2015, taxi_df_2016])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We read the fisrt 6 mounths for each year. Now, we should preprocess data for training:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "query_frags = [\r\n",
    "    'fare_amount > 1 and fare_amount < 500',\r\n",
    "    'passenger_count > 0 and passenger_count < 6',\r\n",
    "    'pickup_longitude > -75 and pickup_longitude < -73',\r\n",
    "    'dropoff_longitude > -75 and dropoff_longitude < -73',\r\n",
    "    'pickup_latitude > 40 and pickup_latitude < 42',\r\n",
    "    'dropoff_latitude > 40 and dropoff_latitude < 42',\r\n",
    "    'trip_distance > 0 and trip_distance < 500',\r\n",
    "    'not (trip_distance > 50 and fare_amount < 50)',\r\n",
    "    'not (trip_distance < 10 and fare_amount > 300)',\r\n",
    "    'not dropoff_datetime <= pickup_datetime'\r\n",
    "]\r\n",
    "taxi_df = taxi_df.query(' and '.join(query_frags))\r\n",
    "taxi_df = taxi_df.reset_index(drop=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "taxi_df['hour'] = taxi_df['pickup_datetime'].dt.hour\r\n",
    "taxi_df['year'] = taxi_df['pickup_datetime'].dt.year\r\n",
    "taxi_df['month'] = taxi_df['pickup_datetime'].dt.month\r\n",
    "taxi_df['day'] = taxi_df['pickup_datetime'].dt.day\r\n",
    "taxi_df['day_of_week'] = taxi_df['pickup_datetime'].dt.weekday\r\n",
    "taxi_df['is_weekend'] = (taxi_df['day_of_week']>=5).astype('int32')\r\n",
    "\r\n",
    "#calculate the time difference between dropoff and pickup.\r\n",
    "taxi_df['diff'] = taxi_df['dropoff_datetime'].astype('int64') - taxi_df['pickup_datetime'].astype('int64')\r\n",
    "taxi_df['diff']=(taxi_df['diff']/1000).astype('int64')\r\n",
    "\r\n",
    "taxi_df['pickup_latitude_r'] = taxi_df['pickup_latitude']//.01*.01\r\n",
    "taxi_df['pickup_longitude_r'] = taxi_df['pickup_longitude']//.01*.01\r\n",
    "taxi_df['dropoff_latitude_r'] = taxi_df['dropoff_latitude']//.01*.01\r\n",
    "taxi_df['dropoff_longitude_r'] = taxi_df['dropoff_longitude']//.01*.01\r\n",
    "\r\n",
    "taxi_df = taxi_df.drop('pickup_datetime', axis=1)\r\n",
    "taxi_df = taxi_df.drop('dropoff_datetime', axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "geo_columns = ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']\r\n",
    "radians = {x: np.radians(taxi_df[x]) for x in geo_columns}\r\n",
    "dlon = radians['pickup_longitude'] - radians['dropoff_longitude']\r\n",
    "dlat = radians['pickup_latitude'] - radians['dropoff_latitude']\r\n",
    "\r\n",
    "taxi_df['h_distance'] = 6367 * 2 * np.arcsin(\r\n",
    "    np.sqrt(\r\n",
    "        np.sin(dlat / 2)**2\r\n",
    "        + np.cos(radians['pickup_latitude']) * np.cos(radians['dropoff_latitude']) * np.sin(dlon / 2)**2))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "taxi_df = taxi_df.drop('trip_distance', axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_train = taxi_df.query('day < 25')\r\n",
    "Y_train = X_train[['fare_amount']]\r\n",
    "X_train = X_train[X_train.columns.difference(['fare_amount'])]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To validate the model, let's split the data on training and test subsets. Then we train and validate model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import train_test_split\r\n",
    "x_train, x_val, y_train, y_val = train_test_split(X_train, Y_train, test_size=0.1, random_state=123)\r\n",
    "print(f\"x_train.shape = {x_train.shape}\")\r\n",
    "print(f\"y_train.shape = {y_train.shape}\")\r\n",
    "print(f\"x_val.shape = {x_val.shape}\")\r\n",
    "print(f\"y_val.shape = {y_val.shape}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model_xgb = xgb.XGBRegressor(tree_method='hist', n_jobs=-1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\r\n",
    "model_xgb = model_xgb.fit(x_train, y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\r\n",
    "xgb_predictions = model_xgb.predict(x_val)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mse = mean_squared_error(y_val, xgb_predictions)\r\n",
    "mse"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see Xgboost works fine with large data, which does not fit into the memory of the GPU.  \r\n",
    "The model trains in a short time with good accuracy."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}