{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting NYC Taxi Fares with RAPIDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAPIDS is a suite of GPU accelerated data science libraries with APIs that should be familiar to users of Pandas, Dask, and Scikitlearn.\n",
    "\n",
    "This notebook focuses on showing how to use cuDF with Dask & XGBoost to scale GPU DataFrame ETL-style operations & model training out to multiple GPUs on mutliple nodes as part of Google Cloud Dataproc.\n",
    "\n",
    "Anaconda has graciously made some of the NYC Taxi dataset available in a public Google Cloud Storage bucket. We'll use our Dataproc Cluster of GPUs to process it and train a model that predicts the fare amount.\n",
    "\n",
    "For EDA we show the examples using [Holoviews](http://holoviews.org/) and [hvplot](https://hvplot.holoviz.org/). Best way to install Holoviews is to from `conda-forge` channel `conda install -c conda-forge holoviews`\n",
    "and for hvplot `pyviz` channel. `conda install -c pyviz hvplot`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import socket, time\n",
    "import modin.pandas as modin_omni_pd\n",
    "# import xgboost as xgb\n",
    "\n",
    "#To install Holoviews and hvplot\n",
    "#conda install -c conda-forge holoviews\n",
    "#conda install -c pyviz hvplot\n",
    "import holoviews as hv\n",
    "from holoviews import opts\n",
    "import numpy as np\n",
    "import hvplot.pandas\n",
    "import hvplot.dask\n",
    "hv.extension('bokeh')\n",
    "import ray\n",
    "ray.init(object_store_memory=42874035712)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspecting the Data\n",
    "\n",
    "Now that we have a cluster of GPU workers, we'll use [dask-cudf](https://github.com/rapidsai/dask-cudf/) to load and parse a bunch of CSV files into a distributed DataFrame. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleanup\n",
    "\n",
    "As usual, the data needs to be massaged a bit before we can start adding features that are useful to an ML model.\n",
    "\n",
    "For example, in the 2014 taxi CSV files, there are `pickup_datetime` and `dropoff_datetime` columns. The 2015 CSVs have `tpep_pickup_datetime` and `tpep_dropoff_datetime`, which are the same columns. One year has `rate_code`, and another `RateCodeID`.\n",
    "\n",
    "Also, some CSV files have column names with extraneous spaces in them.\n",
    "\n",
    "Worst of all, starting in the July 2016 CSVs, pickup & dropoff latitude and longitude data were replaced by location IDs, making the second half of the year useless to us.\n",
    "\n",
    "We'll do a little string manipulation, column renaming, and concatenating of DataFrames to sidestep the problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dictionary of required columns and their datatypes\n",
    "must_haves = {\n",
    "     'pickup_datetime': 'datetime64[s]',\n",
    "     'dropoff_datetime': 'datetime64[s]',\n",
    "     'passenger_count': 'int32',\n",
    "     'trip_distance': 'float32',\n",
    "     'pickup_longitude': 'float32',\n",
    "     'pickup_latitude': 'float32',\n",
    "     'rate_code': 'int32',\n",
    "     'dropoff_longitude': 'float32',\n",
    "     'dropoff_latitude': 'float32',\n",
    "     'fare_amount': 'float32'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(ddf, must_haves):\n",
    "    # replace the extraneous spaces in column names and lower the font type\n",
    "    tmp = {col:col.strip().lower() for col in list(ddf.columns)}\n",
    "    ddf = ddf.rename(columns=tmp)\n",
    "\n",
    "    ddf = ddf.rename(columns={\n",
    "        'tpep_pickup_datetime': 'pickup_datetime',\n",
    "        'tpep_dropoff_datetime': 'dropoff_datetime',\n",
    "        'ratecodeid': 'rate_code'\n",
    "    })\n",
    "    \n",
    "#    ddf['pickup_datetime'] = ddf['pickup_datetime'].astype('datetime64[ms]')\n",
    "#    ddf['dropoff_datetime'] = ddf['dropoff_datetime'].astype('datetime64[ms]')\n",
    "\n",
    "    for col in ddf.columns:\n",
    "        if col not in must_haves:\n",
    "            ddf = ddf.drop(columns=col)\n",
    "            continue\n",
    "        # if column was read as a string, recast as float\n",
    "        if ddf[col].dtype == 'object':\n",
    "            ddf[col] = ddf[col].fillna('-1')\n",
    "#            ddf[col] = ddf[col].astype('float32')\n",
    "#        else:\n",
    "            # downcast from 64bit to 32bit types\n",
    "            # Tesla T4 are faster on 32bit ops\n",
    "#            if 'int' in str(ddf[col].dtype):\n",
    "#                ddf[col] = ddf[col].astype('int32')\n",
    "#            if 'float' in str(ddf[col].dtype):\n",
    "#                ddf[col] = ddf[col].astype('float32')\n",
    "#            ddf[col] = ddf[col].fillna(-1)\n",
    "    \n",
    "    return ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = './nyctaxi/'\n",
    "\n",
    "df_2014 = modin_omni_pd.concat([\n",
    "    clean(modin_omni_pd.read_csv(x, parse_dates=[' pickup_datetime', ' dropoff_datetime']), must_haves)\n",
    "    for x in glob.glob(base_path+'2014/yellow_*.csv')], ignore_index=True)\n",
    "#df_2014 = modin_omni_pd.read_csv(base_path+'2014/yellow_tripdata_2014-01.csv', parse_dates=[' pickup_datetime', ' dropoff_datetime'])\n",
    "#df_2014 = clean(df_2014, must_haves)\n",
    "#df_2014.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pickup_datetime      datetime64[ns]\n",
       "dropoff_datetime     datetime64[ns]\n",
       "passenger_count               int64\n",
       "trip_distance               float64\n",
       "pickup_longitude            float64\n",
       "pickup_latitude             float64\n",
       "rate_code                     int64\n",
       "dropoff_longitude           float64\n",
       "dropoff_latitude            float64\n",
       "fare_amount                 float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2014.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> NOTE: </b>We will realize that some of 2015 data has column name as `RateCodeID` and others have `RatecodeID`. When we rename the columns in the clean function, it internally doesn't pass meta while calling map_partitions(). This leads to the error of column name mismatch in the returned data. For this reason, we will call the clean function with map_partition and pass the meta to it. Here is the link to the bug created for that: https://github.com/rapidsai/cudf/issues/5413 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(165114361, 10)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2014.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still have 2015 and the first half of 2016's data to read and clean. Let's increase our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2015 = modin_omni_pd.concat([\n",
    "    clean(modin_omni_pd.read_csv(x, parse_dates=['tpep_pickup_datetime', 'tpep_dropoff_datetime']), must_haves)\n",
    "    for x in glob.glob(base_path + '2015/yellow_*.csv')], ignore_index=True)\n",
    "#df_2015 = modin_omni_pd.read_csv(base_path+'2015/yellow_tripdata_2015-01.csv', parse_dates=['tpep_pickup_datetime', 'tpep_dropoff_datetime'])\n",
    "#df_2015 = clean(df_2015, must_haves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(146112989, 10)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2015.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling 2016's Mid-Year Schema Change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 2016, only January - June CSVs have the columns we need. If we try to read base_path+2016/yellow_*.csv, Dask will not appreciate having differing schemas in the same DataFrame.\n",
    "\n",
    "Instead, we'll need to create a list of the valid months and read them independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "months = [str(x).rjust(2, '0') for x in range(1, 7)]\n",
    "valid_files = [base_path+'2016/yellow_tripdata_2016-'+month+'.csv' for month in months]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read & clean 2016 data and concat all DFs\n",
    "df_2016 = modin_omni_pd.concat([\n",
    "    clean(modin_omni_pd.read_csv(x, parse_dates=['tpep_pickup_datetime', 'tpep_dropoff_datetime']), must_haves)\n",
    "    for x in valid_files], ignore_index=True)\n",
    "#df_2016 = clean(modin_omni_pd.read_csv(valid_files[0], parse_dates=['tpep_pickup_datetime', 'tpep_dropoff_datetime']), must_haves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate multiple DataFrames into one bigger one\n",
    "taxi_df = modin_omni_pd.concat([df_2014, df_2015, df_2016], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pickup_datetime      datetime64[ns]\n",
       "dropoff_datetime     datetime64[ns]\n",
       "passenger_count               int64\n",
       "trip_distance               float64\n",
       "pickup_longitude            float64\n",
       "pickup_latitude             float64\n",
       "rate_code                     int64\n",
       "dropoff_longitude           float64\n",
       "dropoff_latitude            float64\n",
       "fare_amount                 float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# taxi_df = taxi_df.persist()\n",
    "taxi_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(380633870, 10)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are checking out if there are any non-sensical records and outliers, and in such case, we need to remove them from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>rate_code</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>fare_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3832</th>\n",
       "      <td>2014-01-09 20:27:41</td>\n",
       "      <td>2014-01-09 19:05:21</td>\n",
       "      <td>1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.969591</td>\n",
       "      <td>40.789003</td>\n",
       "      <td>9.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16134</th>\n",
       "      <td>2014-01-09 22:15:40</td>\n",
       "      <td>2014-01-09 22:15:14</td>\n",
       "      <td>1</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>-74.002861</td>\n",
       "      <td>40.760625</td>\n",
       "      <td>7.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16181</th>\n",
       "      <td>2014-01-09 22:27:13</td>\n",
       "      <td>2014-01-09 22:26:49</td>\n",
       "      <td>1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.976287</td>\n",
       "      <td>40.741631</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16873</th>\n",
       "      <td>2014-01-10 00:14:23</td>\n",
       "      <td>2014-01-10 00:14:23</td>\n",
       "      <td>1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-74.005136</td>\n",
       "      <td>40.718694</td>\n",
       "      <td>1</td>\n",
       "      <td>-74.003879</td>\n",
       "      <td>40.728503</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25248</th>\n",
       "      <td>2014-01-10 00:39:16</td>\n",
       "      <td>2014-01-10 00:38:59</td>\n",
       "      <td>1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.999284</td>\n",
       "      <td>40.733897</td>\n",
       "      <td>6.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          pickup_datetime    dropoff_datetime  passenger_count  trip_distance  \\\n",
       "3832  2014-01-09 20:27:41 2014-01-09 19:05:21                1            1.8   \n",
       "16134 2014-01-09 22:15:40 2014-01-09 22:15:14                1            1.3   \n",
       "16181 2014-01-09 22:27:13 2014-01-09 22:26:49                1            1.9   \n",
       "16873 2014-01-10 00:14:23 2014-01-10 00:14:23                1            0.7   \n",
       "25248 2014-01-10 00:39:16 2014-01-10 00:38:59                1            1.5   \n",
       "\n",
       "       pickup_longitude  pickup_latitude  rate_code  dropoff_longitude  \\\n",
       "3832           0.000000         0.000000          1         -73.969591   \n",
       "16134          0.000000         0.000000          1         -74.002861   \n",
       "16181          0.000000         0.000000          1         -73.976287   \n",
       "16873        -74.005136        40.718694          1         -74.003879   \n",
       "25248          0.000000         0.000000          1         -73.999284   \n",
       "\n",
       "       dropoff_latitude  fare_amount  \n",
       "3832          40.789003          9.5  \n",
       "16134         40.760625          7.5  \n",
       "16181         40.741631          8.5  \n",
       "16873         40.728503          4.0  \n",
       "25248         40.733897          6.5  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check out if there is any negative total trip time\n",
    "taxi_df[taxi_df.dropoff_datetime <= taxi_df.pickup_datetime].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>rate_code</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>fare_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>373953</th>\n",
       "      <td>2014-01-12 15:03:38</td>\n",
       "      <td>2014-01-12 15:05:31</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-73.823842</td>\n",
       "      <td>40.690258</td>\n",
       "      <td>5</td>\n",
       "      <td>-73.823847</td>\n",
       "      <td>40.690280</td>\n",
       "      <td>400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520658</th>\n",
       "      <td>2014-01-06 23:44:07</td>\n",
       "      <td>2014-01-06 23:45:01</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-73.927900</td>\n",
       "      <td>41.677242</td>\n",
       "      <td>5</td>\n",
       "      <td>-73.927900</td>\n",
       "      <td>41.677242</td>\n",
       "      <td>370.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525419</th>\n",
       "      <td>2014-01-06 23:50:28</td>\n",
       "      <td>2014-01-06 23:51:40</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-73.927900</td>\n",
       "      <td>41.677242</td>\n",
       "      <td>5</td>\n",
       "      <td>-73.927900</td>\n",
       "      <td>41.677242</td>\n",
       "      <td>370.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562916</th>\n",
       "      <td>2014-01-07 12:09:38</td>\n",
       "      <td>2014-01-07 12:10:27</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-73.929695</td>\n",
       "      <td>40.812032</td>\n",
       "      <td>5</td>\n",
       "      <td>-73.929695</td>\n",
       "      <td>40.812032</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576409</th>\n",
       "      <td>2014-01-07 16:35:10</td>\n",
       "      <td>2014-01-07 16:35:41</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-73.996006</td>\n",
       "      <td>40.720933</td>\n",
       "      <td>5</td>\n",
       "      <td>-73.996007</td>\n",
       "      <td>40.720928</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           pickup_datetime    dropoff_datetime  passenger_count  \\\n",
       "373953 2014-01-12 15:03:38 2014-01-12 15:05:31                1   \n",
       "520658 2014-01-06 23:44:07 2014-01-06 23:45:01                1   \n",
       "525419 2014-01-06 23:50:28 2014-01-06 23:51:40                1   \n",
       "562916 2014-01-07 12:09:38 2014-01-07 12:10:27                1   \n",
       "576409 2014-01-07 16:35:10 2014-01-07 16:35:41                1   \n",
       "\n",
       "        trip_distance  pickup_longitude  pickup_latitude  rate_code  \\\n",
       "373953            0.0        -73.823842        40.690258          5   \n",
       "520658            0.0        -73.927900        41.677242          5   \n",
       "525419            0.0        -73.927900        41.677242          5   \n",
       "562916            0.0        -73.929695        40.812032          5   \n",
       "576409            0.0        -73.996006        40.720933          5   \n",
       "\n",
       "        dropoff_longitude  dropoff_latitude  fare_amount  \n",
       "373953         -73.823847         40.690280        400.0  \n",
       "520658         -73.927900         41.677242        370.0  \n",
       "525419         -73.927900         41.677242        370.0  \n",
       "562916         -73.929695         40.812032        500.0  \n",
       "576409         -73.996007         40.720928        500.0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check out if there is any abnormal data where trip distance is short, but the fare is very high.\n",
    "taxi_df[(taxi_df.trip_distance < 10) & (taxi_df.fare_amount > 300)].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>rate_code</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>fare_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11580</th>\n",
       "      <td>2014-01-09 22:22:14</td>\n",
       "      <td>2014-01-09 22:42:59</td>\n",
       "      <td>1</td>\n",
       "      <td>82.1</td>\n",
       "      <td>-74.002175</td>\n",
       "      <td>40.751753</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.964216</td>\n",
       "      <td>40.719554</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33723</th>\n",
       "      <td>2014-01-10 05:43:48</td>\n",
       "      <td>2014-01-10 06:12:20</td>\n",
       "      <td>1</td>\n",
       "      <td>84.2</td>\n",
       "      <td>-73.965053</td>\n",
       "      <td>40.806673</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.957981</td>\n",
       "      <td>40.713139</td>\n",
       "      <td>34.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122880</th>\n",
       "      <td>2014-01-10 18:16:53</td>\n",
       "      <td>2014-01-10 18:48:53</td>\n",
       "      <td>1</td>\n",
       "      <td>89.9</td>\n",
       "      <td>-73.997185</td>\n",
       "      <td>40.742357</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.948361</td>\n",
       "      <td>40.779271</td>\n",
       "      <td>21.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226264</th>\n",
       "      <td>2014-01-11 13:22:07</td>\n",
       "      <td>2014-01-11 13:40:16</td>\n",
       "      <td>1</td>\n",
       "      <td>81.7</td>\n",
       "      <td>-73.991685</td>\n",
       "      <td>40.759871</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.994985</td>\n",
       "      <td>40.726054</td>\n",
       "      <td>13.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290203</th>\n",
       "      <td>2014-01-11 20:12:50</td>\n",
       "      <td>2014-01-11 20:15:15</td>\n",
       "      <td>1</td>\n",
       "      <td>60.4</td>\n",
       "      <td>-73.968109</td>\n",
       "      <td>40.770878</td>\n",
       "      <td>1</td>\n",
       "      <td>-73.972598</td>\n",
       "      <td>40.764607</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           pickup_datetime    dropoff_datetime  passenger_count  \\\n",
       "11580  2014-01-09 22:22:14 2014-01-09 22:42:59                1   \n",
       "33723  2014-01-10 05:43:48 2014-01-10 06:12:20                1   \n",
       "122880 2014-01-10 18:16:53 2014-01-10 18:48:53                1   \n",
       "226264 2014-01-11 13:22:07 2014-01-11 13:40:16                1   \n",
       "290203 2014-01-11 20:12:50 2014-01-11 20:15:15                1   \n",
       "\n",
       "        trip_distance  pickup_longitude  pickup_latitude  rate_code  \\\n",
       "11580            82.1        -74.002175        40.751753          1   \n",
       "33723            84.2        -73.965053        40.806673          1   \n",
       "122880           89.9        -73.997185        40.742357          1   \n",
       "226264           81.7        -73.991685        40.759871          1   \n",
       "290203           60.4        -73.968109        40.770878          1   \n",
       "\n",
       "        dropoff_longitude  dropoff_latitude  fare_amount  \n",
       "11580          -73.964216         40.719554         20.0  \n",
       "33723          -73.957981         40.713139         34.5  \n",
       "122880         -73.948361         40.779271         21.5  \n",
       "226264         -73.994985         40.726054         13.5  \n",
       "290203         -73.972598         40.764607          4.0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check out if there is any abnormal data where trip distance is long, but the fare is very low.\n",
    "taxi_df[(taxi_df.trip_distance > 50) & (taxi_df.fare_amount < 50)].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using only 2016-01 data for visuals.\n",
    "#taxi_df_cdf = clean(cudf.read_csv(valid_files[0]),must_haves)\n",
    "\n",
    "#Using entire 2016 data for visualization\n",
    "#taxi_df_cdf = taxi_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot below visualizes the histogram of trip_distance and we can see some abnormal trip_distance values for some records. Taking this and also the NYC map coordinates into consideration, we will only select records where tripdistance < 500 miles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 5.48 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Histogram using cupy and Holoviews\n",
    "# frequencies, edges = cupy.histogram(x=cupy.array(taxi_df_cdf[\"trip_distance\"]) , bins=20)\n",
    "# hist = hv.Histogram((np.array(edges.tolist()), np.array(frequencies.tolist())))\n",
    "\n",
    "#Histogram using hvplot\n",
    "#hist = taxi_df_cdf._to_pandas().hvplot.hist(\"trip_distance\", bins=20, bin_range=(0, 10))\n",
    "\n",
    "#Customizing the plot\n",
    "#hist.opts(xlabel=\"trip distance (miles)\",ylabel=\"count\",color=\"green\",width=900, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, the plot below visualizes the histogram of fare_amount and we can see some abnormal fare_amount values for some records. Taking this and also the NYC map coordinates into consideration, we will only select records where fare_amount < 500$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 4.77 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Histogram using cupy and Holoviews\n",
    "# frequencies, edges = cupy.histogram(x=cupy.array(taxi_df_cdf[\"fare_amount\"]) , bins=20)\n",
    "# hist = hv.Histogram((np.array(edges.tolist()), np.array(frequencies.tolist())))\n",
    "\n",
    "#Histogram using hvplot\n",
    "#hist = taxi_df_cdf._to_pandas().hvplot.hist(\"fare_amount\", bins=20, bin_range=(0, 50))\n",
    "\n",
    "#Customizing the plot\n",
    "#hist.opts(xlabel=\"fare amount ($)\",ylabel=\"count\",color=\"green\",width=900, height=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 4.53 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Plot the number of passengers per trip. We'll remove the records where passenger_count > 5.\n",
    "# Plotting using Holoviews\n",
    "#bar = hv.Bars(taxi_df_cdf.groupby(\"passenger_count\").size().to_frame().rename(columns={0:\"count\"}))\n",
    "\n",
    "# Plotting using hvplot\n",
    "#df_bar = taxi_df_cdf.groupby(\"passenger_count\").size().to_frame().rename(columns={0:\"count\"}).reset_index()\n",
    "#bar = df_bar._to_pandas().hvplot.bar(x=\"passenger_count\",y=\"count\")\n",
    "\n",
    "#Customizing the plot\n",
    "#bar.opts(color=\"green\",width=900, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EDA visuals and additional analysis yield the filter logic below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(380633870, 10)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply a list of filter conditions to throw out records with missing or outlier values\n",
    "taxi_df = taxi_df[\n",
    "    (taxi_df.fare_amount > 1) &\n",
    "    (taxi_df.fare_amount < 500) &\n",
    "    (taxi_df.passenger_count > 0) &\n",
    "    (taxi_df.passenger_count < 6) &\n",
    "    (taxi_df.pickup_longitude > -75) &\n",
    "    (taxi_df.pickup_longitude < -73) &\n",
    "    (taxi_df.dropoff_longitude > -75) &\n",
    "    (taxi_df.dropoff_longitude < -73) &\n",
    "    (taxi_df.pickup_latitude > 40) &\n",
    "    (taxi_df.pickup_latitude < 42) &\n",
    "    (taxi_df.dropoff_latitude > 40) &\n",
    "    (taxi_df.dropoff_latitude < 42) &\n",
    "    (taxi_df.trip_distance > 0) &\n",
    "    (taxi_df.trip_distance < 500) &\n",
    "    ((taxi_df.trip_distance <= 50) | (taxi_df.fare_amount >= 50)) &\n",
    "    ((taxi_df.trip_distance >= 10) | (taxi_df.fare_amount <= 300)) &\n",
    "    (taxi_df.dropoff_datetime > taxi_df.pickup_datetime)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset_index and drop index column\n",
    "taxi_df = taxi_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Interesting Features\n",
    "\n",
    "Dask & cuDF provide standard DataFrame operations, but also let you run \"user defined functions\" on the underlying data. Here we use [dask.dataframe's map_partitions](https://docs.dask.org/en/latest/dataframe-api.html#dask.dataframe.DataFrame.map_partitions) to apply user defined python function on each DataFrame partition.\n",
    "\n",
    "We'll use a Haversine Distance calculation to find total trip distance, and extract additional useful variables from the datetime fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add features\n",
    "\n",
    "#taxi_df['hour'] = taxi_df['pickup_datetime'].dt.hour\n",
    "#taxi_df['year'] = taxi_df['pickup_datetime'].dt.year\n",
    "#taxi_df['month'] = taxi_df['pickup_datetime'].dt.month\n",
    "taxi_df['day'] = taxi_df['pickup_datetime'].dt.day\n",
    "#taxi_df['day_of_week'] = taxi_df['pickup_datetime'].dt.weekday\n",
    "#taxi_df['is_weekend'] = (taxi_df['day_of_week']>=5).astype('int32')\n",
    "\n",
    "#calculate the time difference between dropoff and pickup.\n",
    "taxi_df['diff'] = taxi_df['dropoff_datetime'].astype('int64') - taxi_df['pickup_datetime'].astype('int64')\n",
    "taxi_df['diff']=(taxi_df['diff']/1000).astype('int64')\n",
    "\n",
    "taxi_df['pickup_latitude_r'] = taxi_df['pickup_latitude']//.01*.01\n",
    "taxi_df['pickup_longitude_r'] = taxi_df['pickup_longitude']//.01*.01\n",
    "taxi_df['dropoff_latitude_r'] = taxi_df['dropoff_latitude']//.01*.01\n",
    "taxi_df['dropoff_longitude_r'] = taxi_df['dropoff_longitude']//.01*.01\n",
    "\n",
    "taxi_df = taxi_df.drop('pickup_datetime', axis=1)\n",
    "taxi_df = taxi_df.drop('dropoff_datetime', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(358360330, 14)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#geo_columns = ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']\n",
    "#radians = {x: np.radians(taxi_df[x]) for x in geo_columns}\n",
    "#dlon = radians['pickup_longitude'] - radians['dropoff_longitude']\n",
    "#dlat = radians['pickup_latitude'] - radians['dropoff_latitude']\n",
    "\n",
    "#taxi_df['h_distance'] = 6367 * 2 * np.arcsin(\n",
    "#    np.sqrt(\n",
    "#        np.sin(dlat / 2)**2\n",
    "#        + np.cos(radians['pickup_latitude']) * np.cos(radians['dropoff_latitude']) * np.sin(dlon / 2)**2))\n",
    "\n",
    "dlon = taxi_df['dropoff_longitude'] - taxi_df['pickup_longitude']\n",
    "dlat = taxi_df['dropoff_latitude'] - taxi_df['pickup_latitude']\n",
    "taxi_df['m_distance'] = dlon * dlon + dlat * dlat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "passenger_count          int64\n",
       "trip_distance          float64\n",
       "pickup_longitude       float64\n",
       "pickup_latitude        float64\n",
       "rate_code                int64\n",
       "dropoff_longitude      float64\n",
       "dropoff_latitude       float64\n",
       "fare_amount            float64\n",
       "day                      int64\n",
       "diff                     int64\n",
       "pickup_latitude_r      float64\n",
       "pickup_longitude_r     float64\n",
       "dropoff_latitude_r     float64\n",
       "dropoff_longitude_r    float64\n",
       "m_distance             float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(358360330, 15)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxi_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pick a Training Set\n",
    "\n",
    "Let's imagine you're making a trip to New York on the 25th and want to build a model to predict what fare prices will be like the last few days of the month based on the first part of the month. We'll use a query expression to identify the `day` of the month to use to divide the data into train and test sets.\n",
    "\n",
    "The wall-time below represents how long it takes your GPU cluster to load data from the Google Cloud Storage bucket and the ETL portion of the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#since we calculated the h_distance let's drop the trip_distance column, and then do model training with XGB.\n",
    "taxi_df = taxi_df.drop('trip_distance', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the original data partition for train and test sets.\n",
    "X_train = taxi_df[taxi_df.day < 25]\n",
    "\n",
    "# create a Y_train ddf with just the target variable\n",
    "Y_train = X_train[['fare_amount']]\n",
    "# drop the target variable from the training ddf\n",
    "X_train = X_train[X_train.columns.difference(['fare_amount'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the XGBoost Regression Model\n",
    "\n",
    "The wall time output below indicates how long it took your GPU cluster to train an XGBoost model over the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "trained_model = xgb.train({\n",
    "    'learning_rate': 0.3,\n",
    "    'max_depth': 8,\n",
    "    'objective': 'reg:squarederror',\n",
    "    'subsample': 0.6,\n",
    "    'gamma': 1,\n",
    "    'silent': True,\n",
    "    'verbose_eval': True,\n",
    "    'tree_method':'hist'\n",
    "    },\n",
    "    dtrain,\n",
    "    num_boost_round=100, evals=[(dtrain, 'train')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = xgb.plot_importance(trained_model, height=0.8, max_num_features=10, importance_type=\"gain\")\n",
    "ax.grid(False, axis=\"y\")\n",
    "ax.set_title('Estimated feature importance')\n",
    "ax.set_xlabel('importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How Good is Our Model?\n",
    "\n",
    "Now that we have a trained model, we need to test it with the 25% of records we held out.\n",
    "\n",
    "Based on the filtering conditions applied to this dataset, many of the DataFrame partitions will wind up having 0 rows. This is a problem for XGBoost which doesn't know what to do with 0 length arrays. We'll repartition the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = taxi_df[taxi_df.day >= 25]\n",
    "\n",
    "# Create Y_test with just the fare amount\n",
    "Y_test = X_test[['fare_amount']]\n",
    "\n",
    "# Drop the fare amount from X_test\n",
    "X_test = X_test[X_test.columns.difference(['fare_amount'])]\n",
    "\n",
    "# display test set size\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate predictions on the test set\n",
    "'''feed X_test as a dask.dataframe'''\n",
    "\n",
    "booster = trained_model\n",
    "# history = trained_model['history'] # \"History\" is a dictionary containing evaluation results \n",
    "\n",
    "# booster.set_param({'predictor': 'gpu_predictor'})\n",
    "\n",
    "prediction = modin_omni_pd.Series(booster.predict(xgb.DMatrix(X_test)))\n",
    "\n",
    "prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction = prediction.map_partitions(lambda part: cudf.Series(part)).reset_index(drop=True)\n",
    "actual = Y_test['fare_amount'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: We mapped each partition of the result from `xgb.dask.predict` into `cudf.Series` to be able to substract it from `actual` data. Here is the issue asking XGBoost to solve that before returning data from `xgb.dask.predict` https://github.com/dmlc/xgboost/issues/5823#issuecomment-648526888"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate RMSE\n",
    "squared_error = ((prediction-actual)**2)\n",
    "\n",
    "# compute the actual RMSE over the full test set\n",
    "np.sqrt(squared_error.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "scaler_x = MinMaxScaler()\n",
    "scaler_y = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_x.fit(X_train)\n",
    "X_train = scaler_x.transform(X_train)\n",
    "X_test = scaler_x.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_y.fit(Y_train.to_numpy().reshape(-1, 1))\n",
    "Y_train = scaler_y.transform(Y_train.to_numpy().reshape(-1, 1)).ravel()\n",
    "Y_test = scaler_y.transform(Y_test.to_numpy().reshape(-1, 1)).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Ridge regression model with  Intel® Extension for Scikit-learn*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_r_p = Ridge(random_state=123)\n",
    "model_r_p.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Ridge regression model without  Intel® Extension for Scikit-learn*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearnex import unpatch_sklearn\n",
    "unpatch_sklearn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_r_s = Ridge(random_state=123)\n",
    "model_r_s.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's look MSE metric of Our Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_p = model_r_p.predict(X_test)\n",
    "y_pred_s = model_r_s.predict(X_test)\n",
    "\n",
    "mse_r_p = mean_squared_error(Y_test, y_pred_p)\n",
    "mse_r_s = mean_squared_error(Y_test, y_pred_s)\n",
    "\n",
    "print(f'MSE of pached Ridge: {mse_r_p}')\n",
    "print(f'MSE of unpached Ridge: {mse_r_s}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the model with Intel® Extension for Scikit-learn * learns much faster and has the same mse."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
